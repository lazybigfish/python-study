# 爬虫基础知识

关于requests库 的使用

response 对象的四个属性

- response.status_code  检查请求是否成功

```python
res = requests.get('url')
print(res.status_code)
```



常见的几种代码反馈

![image-20190729221011398](/Users/zhuguolin/Library/Application Support/typora-user-images/image-20190729221011398.png)

- Response.content  以二进制的形式将数据返回，适用于图片，音视频的下载

```
import requests
res = requests.get('https://res.pandateacher.com/2018-12-18-10-43-07.png')
#发出请求，并把返回的结果放在变量res中
pic=res.content
#把Reponse对象的内容以二进制数据的形式返回
photo = open('ppt.jpg','wb')
#新建了一个文件ppt.jpg，这里的文件没加路径，它会被保存在程序运行的当前目录下。
#图片内容需要以二进制wb读写。你在学习open()函数时接触过它。
photo.write(pic) 
#获取pic的二进制内容
photo.close()
#关闭文件
```

- Response.text 将内容以字符串的形式返回，适用于源代码文字下载  

```
import requests
#引用requests库
res = requests.get('https://localprod.pandateacher.com/python-manuscript/crawler-html/sanguo.md')
#下载《三国演义》第一回，我们得到一个对象，它被命名为res
novel=res.text
#把Response对象的内容以字符串的形式返回
print(novel[:800])
#现在，可以打印小说了，但考虑到整章太长，只输出800字看看就好。在关于列表的知识那里，你学过[:800]的用法。
```

- Response.encoding  定义response的编码类型

那在真实的情况中，我们该在什么时候用res.encoding呢？

首先，目标数据本身是什么编码是未知的。用requests.get()发送请求后，我们会取得一个Response对象，其中，requests库会对数据的编码类型做出自己的判断。但是！这个判断有可能准确，也可能不准确。

如果它判断准确的话，我们打印出来的response.text的内容就是正常的、没有乱码的，那就用不到res.encoding；如果判断不准确，就会出现一堆乱码，那我们就可以去查看目标数据的编码，然后再用res.encoding把编码定义成和目标数据一致的类型即可。

总的来说，就是遇上文本的乱码问题，才考虑用res.encoding。

## 关于beautifulsoup库的使用

解析数据

```
from bs4 import BeautifulSoup
soup = BeautifulSoup(str,'html.parser')
```

提取数据

find()  /  find_all()

![image-20190730123748840](/Users/zhuguolin/Library/Application Support/typora-user-images/image-20190730123748840.png)

Find_all  查到数据后是一个列表，要遍历一边成为tag对象

Tag 对象

![image-20190730125220393](/Users/zhuguolin/Library/Application Support/typora-user-images/image-20190730125220393.png)

爬虫的基本操作原理

先抓取全部数据-为一个列表

将数据通过for循环进行遍历成为tag对象。

继续在for循环中将tag对象解析

打印输出结果

![image-20190730130449997](/Users/zhuguolin/Library/Application Support/typora-user-images/image-20190730130449997.png)